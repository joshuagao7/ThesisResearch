\newpage
\section{Introduction}\label{sec1}

Athletic training has become increasingly data-driven, with sports science workflows that involve vast quantities of data analysis. Various types of raw data are collected to assess performance, including data from video, GPS, or force plates. In recent years, significant interest has been placed on analyzing ground reaction force (GRF) data; every movement that an athlete makes must be counteracted by forces from the ground. Studies have shown that GRF data can be sufficiently high dimensional to recreate data collected from other types of sensors like video cameras.\footnote{Related work includes the MIT intelligent carpet system, which uses pressure-sensing technology to infer 3D human pose from tactile signals. See Y. Liu et al., ``Intelligent Carpet: Inferring 3D Human Pose from Tactile Signals,'' CVPR 2021.}

A challenge with early pressure plate data is that raw data can be noisy, uncalibrated, and at such high quantities, difficult to analyze manually. If future investment to collect comprehensive athlete performance data from GRF sensors increases, this problem will only become more prevalent. This paper works with GRF data from a 48-sensor capacitive pressure plate sampled at ~50Hz. The sensors are arranged in a 6x8 grid, providing spatial resolution of force distribution across the pressure plate as people perform vertical jumps.

To any trained human analyst, it is easy to identify jumps in GRF data. We can see the force build-up during countermovement and propulsion, the rapid unloading at takeoff, the near-zero force while airborne, and the rapid reloading at landing. This paper seeks to encode this human intuition into deterministic algorithms that can be used to detect jumps in GRF data. We think of it similarly to the MNIST handwritten digit recognition challenge: we collected a labeled dataset of 279 jumps, then developed various generalizable algorithms to detect jumps in the data. We borrow concepts from machine learning (e.g. loss landscape traversal, parameter tuning) to streamline the process, and the goal is to maintain interpretable, deterministic algorithms.

Raw data is loaded into matrix $\mathbf{P} \in \mathbb{R}^{T \times 48}$, where $T$ is the number of time samples. Information from individual sensor readings can provide granular insights into how weight is shifted across various parts of ground contact to generate propulsion and absorption. For detecting vertical jump events in this paper, we use the pooled signal. Individual sensors record pressure $P_i = F_i / A_i$ (force per unit area), where $F_i$ is the force on sensor $i$ and $A_i$ is its area. Pooling these pressure measurements gives a signal related to total force: $\sum_{i=1}^{48} P_i = \sum_{i=1}^{48} F_i / A_i \propto F(t)$. Pooling the sensor signal also smooths out idiosyncratic sensor noise. 

\begin{equation}
F(t) = \sum_{i=1}^{48} P_{t,i}
\end{equation}

Figure~\ref{fig:raw_with_annotations} illustrates the data structure and the detection challenge. The figure shows three key elements: (1) \textbf{Raw sensor data}: Each thin line represents a unique sensor from the 48-sensor array, capturing individual pressure measurements over time. (2) \textbf{Pooled signal}: The thick blue line represents the pooled sensor values. Note that this signal is not calibrated to absolute force units; it is proportional to force by some factor, possibly nonlinear, depending on sensor characteristics and manufacturing variations. (3) \textbf{Ground truth markers}: The vertical dashed lines indicate manually annotated jump events---markings where jumps actually happened. Our goal is to automatically detect these events using deterministic algorithms.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{../results/plots/presentation/raw_with_annotations.png}
\caption{Raw sensor data with ground truth jump markers. Each colored line represents a unique sensor from the 48-sensor array. The thick blue line shows the pooled signal. Vertical dashed lines indicate manually annotated jump events that we aim to detect automatically.}
\label{fig:raw_with_annotations}
\end{figure}

We propose a framework that mimics human intuition for pattern classification. We encode human jump detection qualitative cues into interpretable signal-processing algorithms: (1) \textbf{threshold-based}: identifies flight periods when force falls below a threshold; (2) \textbf{derivative-based}: detects paired takeoff and landing events in the derivative signal; (3) \textbf{correlation-based}: searches for a characteristic signal within the pooled signal resembling a jump by matching buffer regions of the signal with a pre-specified template; (4) \textbf{landing derivative}: focuses exclusively on landing detection, with jump centers calculated by offsetting backward from landing points, and (5) \textbf{ensemble}: integrates multiple algorithms through weighted voting on binary condition indicators. All algorithms are evaluated using the same loss function and parameter optimization framework, enabling direct performance comparison. All algorithms can operate in $O(T)$ time, enabling real-time practical deployment.